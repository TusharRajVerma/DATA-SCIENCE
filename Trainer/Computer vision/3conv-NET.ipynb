{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-75ddb4cc757c>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/tushar/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/tushar/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting temp/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/tushar/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting temp/train-labels-idx1-ubyte.gz\n",
      "Extracting temp/t10k-images-idx3-ubyte.gz\n",
      "Extracting temp/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/tushar/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'temp'\n",
    "mnist = read_data_sets(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xdata = np.array([np.reshape(x, (28,28)) for x in mnist.train.images])\n",
    "test_xdata = np.array([np.reshape(x, (28,28)) for x in mnist.test.images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = mnist.train.labels\n",
    "test_labels = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "learning_rate = 0.005\n",
    "evaluation_size = 500\n",
    "image_width = train_xdata[0].shape[0]\n",
    "image_height = train_xdata[0].shape[1]\n",
    "target_size = max(train_labels) + 1\n",
    "num_channels = 1 # greyscale = 1 channel\n",
    "generations = 500\n",
    "eval_every = 5\n",
    "conv1_features = 128\n",
    "conv2_features = 128\n",
    "conv3_features = 128\n",
    "max_pool_size1 = 2 # NxN window for 1st max pool layer\n",
    "max_pool_size2 = 2\n",
    "max_pool_size3 = 2# NxN window for 2nd max pool layer\n",
    "fully_connected_size1 = 100\n",
    "fully_connected_size2 = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare model placeholders\n",
    "x_input_shape = (batch_size, image_width, image_height, num_channels)\n",
    "x_input = tf.placeholder(tf.float32, shape=x_input_shape)\n",
    "y_target = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "eval_input_shape = (evaluation_size, image_width, image_height, num_channels)\n",
    "eval_input = tf.placeholder(tf.float32, shape=eval_input_shape)\n",
    "eval_target = tf.placeholder(tf.int32, shape=(evaluation_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_weight = tf.Variable(tf.truncated_normal([4, 4, num_channels, conv1_features],\n",
    "                                               stddev=0.1, dtype=tf.float32))\n",
    "conv1_bias = tf.Variable(tf.zeros([conv1_features], dtype=tf.float32))\n",
    "\n",
    "conv2_weight = tf.Variable(tf.truncated_normal([4, 4, conv1_features, conv2_features],\n",
    "                                               stddev=0.1, dtype=tf.float32))\n",
    "conv2_bias = tf.Variable(tf.zeros([conv2_features], dtype=tf.float32))\n",
    "conv3_weight = tf.Variable(tf.truncated_normal([4, 4, conv2_features, conv3_features],\n",
    "                                               stddev=0.1, dtype=tf.float32))\n",
    "conv3_bias = tf.Variable(tf.zeros([conv3_features], dtype=tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "resulting_width = 1+(image_width // (max_pool_size1 * max_pool_size2 * max_pool_size3))\n",
    "resulting_height = 1+(image_height // (max_pool_size1 * max_pool_size2 *max_pool_size3))\n",
    "full1_input_size = resulting_width * resulting_height * conv3_features\n",
    "full1_weight = tf.Variable(tf.truncated_normal([full1_input_size, fully_connected_size1],\n",
    "                          stddev=0.1, dtype=tf.float32))\n",
    "full1_bias = tf.Variable(tf.truncated_normal([fully_connected_size1], stddev=0.1, dtype=tf.float32))\n",
    "full2_weight = tf.Variable(tf.truncated_normal([fully_connected_size1, fully_connected_size2],\n",
    "                                               stddev=0.1, dtype=tf.float32))\n",
    "full2_bias = tf.Variable(tf.truncated_normal([fully_connected_size2], stddev=0.1, dtype=tf.float32))\n",
    "full3_weight = tf.Variable(tf.truncated_normal([fully_connected_size2, target_size],\n",
    "                                               stddev=0.1, dtype=tf.float32))\n",
    "full3_bias = tf.Variable(tf.truncated_normal([target_size], stddev=0.1, dtype=tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_conv_net(input_data):\n",
    "    # First Conv-ReLU-MaxPool Layer\n",
    "    conv1 = tf.nn.conv2d(input_data, conv1_weight, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_bias))\n",
    "    max_pool1 = tf.nn.max_pool(relu1, ksize=[1, max_pool_size1, max_pool_size1, 1],\n",
    "                               strides=[1, max_pool_size1, max_pool_size1, 1], padding='SAME')\n",
    "\n",
    "    # Second Conv-ReLU-MaxPool Layer\n",
    "    conv2 = tf.nn.conv2d(max_pool1, conv2_weight, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_bias))\n",
    "    max_pool2 = tf.nn.max_pool(relu2, ksize=[1, max_pool_size2, max_pool_size2, 1],\n",
    "                               strides=[1, max_pool_size2, max_pool_size2, 1], padding='SAME')\n",
    "    # Third Conv-ReLU-MaxPool Layer\n",
    "    conv3 = tf.nn.conv2d(max_pool2, conv3_weight, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    relu3 = tf.nn.relu(tf.nn.bias_add(conv3, conv3_bias))\n",
    "    max_pool3 = tf.nn.max_pool(relu3, ksize=[1, max_pool_size3, max_pool_size3, 1],\n",
    "                               strides=[1, max_pool_size3, max_pool_size3, 1], padding='SAME')\n",
    "\n",
    "\n",
    "    # Transform Output into a 1xN layer for next fully connected layer\n",
    "    \n",
    "    final_conv_shape = max_pool3.get_shape().as_list()\n",
    "    final_shape = final_conv_shape[1] * final_conv_shape[2] * final_conv_shape[3]\n",
    "    flat_output = tf.reshape(max_pool3, [final_conv_shape[0], final_shape])\n",
    "\n",
    "    # First Fully Connected Layer\n",
    "    fully_connected1 = tf.nn.relu(tf.add(tf.matmul(flat_output, full1_weight), full1_bias))\n",
    "\n",
    "    # Second Fully Connected Layer\n",
    "    fully_connected2 = tf.nn.relu(tf.add(tf.matmul(fully_connected1, full2_weight), full2_bias))\n",
    "    # Third Fully Connected Layer\n",
    "    final_model_output = tf.add(tf.matmul(fully_connected2, full2_weight), full2_bias)\n",
    "    \n",
    "    return(final_model_output)\n",
    "\n",
    "model_output = my_conv_net(x_input)\n",
    "test_model_output = my_conv_net(eval_input)\n",
    "\n",
    "# Declare Loss Function (softmax cross entropy)\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits = model_output,labels = y_target))\n",
    "\n",
    "# Create a prediction function\n",
    "prediction = tf.nn.softmax(model_output)\n",
    "test_prediction = tf.nn.softmax(test_model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create accuracy function\n",
    "def get_accuracy(logits, targets):\n",
    "    batch_predictions = np.argmax(logits, axis=1)\n",
    "    num_correct = np.sum(np.equal(batch_predictions, targets))\n",
    "    return(100. * num_correct/batch_predictions.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an optimizer\n",
    "my_optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)\n",
    "train_step = my_optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tushar/.local/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Variables\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation # 5. Train Loss: 3.17. Train Acc (Test Acc): 11.00 (9.80)\n",
      "Generation # 10. Train Loss: 1.86. Train Acc (Test Acc): 39.00 (29.20)\n",
      "Generation # 15. Train Loss: 1.76. Train Acc (Test Acc): 42.00 (32.20)\n",
      "Generation # 20. Train Loss: 1.46. Train Acc (Test Acc): 50.00 (54.20)\n",
      "Generation # 25. Train Loss: 1.25. Train Acc (Test Acc): 59.00 (66.20)\n",
      "Generation # 30. Train Loss: 0.89. Train Acc (Test Acc): 72.00 (74.60)\n",
      "Generation # 35. Train Loss: 0.75. Train Acc (Test Acc): 79.00 (74.40)\n",
      "Generation # 40. Train Loss: 0.48. Train Acc (Test Acc): 84.00 (79.80)\n",
      "Generation # 45. Train Loss: 0.58. Train Acc (Test Acc): 84.00 (82.20)\n",
      "Generation # 50. Train Loss: 0.37. Train Acc (Test Acc): 88.00 (87.80)\n",
      "Generation # 55. Train Loss: 0.29. Train Acc (Test Acc): 92.00 (89.80)\n",
      "Generation # 60. Train Loss: 0.27. Train Acc (Test Acc): 91.00 (87.80)\n",
      "Generation # 65. Train Loss: 0.22. Train Acc (Test Acc): 93.00 (85.40)\n",
      "Generation # 70. Train Loss: 0.30. Train Acc (Test Acc): 88.00 (91.80)\n",
      "Generation # 75. Train Loss: 0.33. Train Acc (Test Acc): 87.00 (93.40)\n",
      "Generation # 80. Train Loss: 0.34. Train Acc (Test Acc): 92.00 (90.60)\n",
      "Generation # 85. Train Loss: 0.13. Train Acc (Test Acc): 97.00 (93.20)\n",
      "Generation # 90. Train Loss: 0.26. Train Acc (Test Acc): 92.00 (94.00)\n",
      "Generation # 95. Train Loss: 0.27. Train Acc (Test Acc): 93.00 (95.80)\n",
      "Generation # 100. Train Loss: 0.22. Train Acc (Test Acc): 94.00 (93.20)\n",
      "Generation # 105. Train Loss: 0.20. Train Acc (Test Acc): 92.00 (94.40)\n",
      "Generation # 110. Train Loss: 0.19. Train Acc (Test Acc): 95.00 (90.80)\n",
      "Generation # 115. Train Loss: 0.15. Train Acc (Test Acc): 95.00 (92.00)\n",
      "Generation # 120. Train Loss: 0.15. Train Acc (Test Acc): 97.00 (94.20)\n",
      "Generation # 125. Train Loss: 0.18. Train Acc (Test Acc): 96.00 (92.60)\n",
      "Generation # 130. Train Loss: 0.18. Train Acc (Test Acc): 94.00 (96.20)\n",
      "Generation # 135. Train Loss: 0.12. Train Acc (Test Acc): 98.00 (96.80)\n",
      "Generation # 140. Train Loss: 0.18. Train Acc (Test Acc): 97.00 (95.80)\n",
      "Generation # 145. Train Loss: 0.16. Train Acc (Test Acc): 95.00 (95.20)\n",
      "Generation # 150. Train Loss: 0.22. Train Acc (Test Acc): 94.00 (96.60)\n",
      "Generation # 155. Train Loss: 0.14. Train Acc (Test Acc): 97.00 (94.20)\n",
      "Generation # 160. Train Loss: 0.12. Train Acc (Test Acc): 96.00 (96.20)\n",
      "Generation # 165. Train Loss: 0.08. Train Acc (Test Acc): 97.00 (95.00)\n",
      "Generation # 170. Train Loss: 0.12. Train Acc (Test Acc): 96.00 (94.60)\n",
      "Generation # 175. Train Loss: 0.09. Train Acc (Test Acc): 98.00 (93.80)\n",
      "Generation # 180. Train Loss: 0.21. Train Acc (Test Acc): 96.00 (95.60)\n",
      "Generation # 185. Train Loss: 0.18. Train Acc (Test Acc): 95.00 (90.20)\n",
      "Generation # 190. Train Loss: 0.11. Train Acc (Test Acc): 96.00 (94.80)\n",
      "Generation # 195. Train Loss: 0.09. Train Acc (Test Acc): 96.00 (93.60)\n",
      "Generation # 200. Train Loss: 0.14. Train Acc (Test Acc): 97.00 (94.80)\n",
      "Generation # 205. Train Loss: 0.18. Train Acc (Test Acc): 96.00 (96.00)\n",
      "Generation # 210. Train Loss: 0.07. Train Acc (Test Acc): 99.00 (96.80)\n",
      "Generation # 215. Train Loss: 0.10. Train Acc (Test Acc): 98.00 (95.60)\n",
      "Generation # 220. Train Loss: 0.11. Train Acc (Test Acc): 96.00 (94.80)\n",
      "Generation # 225. Train Loss: 0.17. Train Acc (Test Acc): 94.00 (95.20)\n",
      "Generation # 230. Train Loss: 0.11. Train Acc (Test Acc): 96.00 (97.00)\n",
      "Generation # 235. Train Loss: 0.10. Train Acc (Test Acc): 98.00 (96.00)\n",
      "Generation # 240. Train Loss: 0.15. Train Acc (Test Acc): 96.00 (95.60)\n",
      "Generation # 245. Train Loss: 0.07. Train Acc (Test Acc): 97.00 (92.80)\n",
      "Generation # 250. Train Loss: 0.09. Train Acc (Test Acc): 97.00 (96.80)\n",
      "Generation # 255. Train Loss: 0.16. Train Acc (Test Acc): 94.00 (96.00)\n",
      "Generation # 260. Train Loss: 0.11. Train Acc (Test Acc): 98.00 (97.00)\n",
      "Generation # 265. Train Loss: 0.03. Train Acc (Test Acc): 100.00 (98.20)\n",
      "Generation # 270. Train Loss: 0.09. Train Acc (Test Acc): 97.00 (97.40)\n",
      "Generation # 275. Train Loss: 0.06. Train Acc (Test Acc): 98.00 (95.40)\n",
      "Generation # 280. Train Loss: 0.08. Train Acc (Test Acc): 97.00 (96.00)\n",
      "Generation # 285. Train Loss: 0.16. Train Acc (Test Acc): 95.00 (95.20)\n",
      "Generation # 290. Train Loss: 0.15. Train Acc (Test Acc): 97.00 (95.80)\n",
      "Generation # 295. Train Loss: 0.10. Train Acc (Test Acc): 98.00 (97.40)\n",
      "Generation # 300. Train Loss: 0.13. Train Acc (Test Acc): 98.00 (97.00)\n",
      "Generation # 305. Train Loss: 0.07. Train Acc (Test Acc): 98.00 (96.00)\n",
      "Generation # 310. Train Loss: 0.08. Train Acc (Test Acc): 98.00 (96.80)\n",
      "Generation # 315. Train Loss: 0.12. Train Acc (Test Acc): 96.00 (95.60)\n",
      "Generation # 320. Train Loss: 0.08. Train Acc (Test Acc): 97.00 (96.60)\n",
      "Generation # 325. Train Loss: 0.05. Train Acc (Test Acc): 99.00 (96.40)\n",
      "Generation # 330. Train Loss: 0.04. Train Acc (Test Acc): 98.00 (97.60)\n",
      "Generation # 335. Train Loss: 0.14. Train Acc (Test Acc): 95.00 (98.20)\n",
      "Generation # 340. Train Loss: 0.15. Train Acc (Test Acc): 96.00 (96.80)\n",
      "Generation # 345. Train Loss: 0.07. Train Acc (Test Acc): 99.00 (98.00)\n",
      "Generation # 350. Train Loss: 0.07. Train Acc (Test Acc): 99.00 (96.60)\n",
      "Generation # 355. Train Loss: 0.09. Train Acc (Test Acc): 96.00 (95.20)\n",
      "Generation # 360. Train Loss: 0.13. Train Acc (Test Acc): 96.00 (96.40)\n",
      "Generation # 365. Train Loss: 0.03. Train Acc (Test Acc): 98.00 (97.20)\n",
      "Generation # 370. Train Loss: 0.10. Train Acc (Test Acc): 96.00 (97.00)\n",
      "Generation # 375. Train Loss: 0.11. Train Acc (Test Acc): 97.00 (96.00)\n",
      "Generation # 380. Train Loss: 0.05. Train Acc (Test Acc): 99.00 (96.20)\n",
      "Generation # 385. Train Loss: 0.09. Train Acc (Test Acc): 97.00 (96.20)\n",
      "Generation # 390. Train Loss: 0.12. Train Acc (Test Acc): 96.00 (95.80)\n",
      "Generation # 395. Train Loss: 0.12. Train Acc (Test Acc): 95.00 (97.80)\n",
      "Generation # 400. Train Loss: 0.12. Train Acc (Test Acc): 97.00 (96.40)\n",
      "Generation # 405. Train Loss: 0.08. Train Acc (Test Acc): 98.00 (97.80)\n",
      "Generation # 410. Train Loss: 0.19. Train Acc (Test Acc): 94.00 (96.40)\n",
      "Generation # 415. Train Loss: 0.13. Train Acc (Test Acc): 95.00 (97.80)\n",
      "Generation # 420. Train Loss: 0.09. Train Acc (Test Acc): 96.00 (96.20)\n",
      "Generation # 425. Train Loss: 0.06. Train Acc (Test Acc): 98.00 (95.80)\n",
      "Generation # 430. Train Loss: 0.04. Train Acc (Test Acc): 99.00 (97.40)\n",
      "Generation # 435. Train Loss: 0.04. Train Acc (Test Acc): 100.00 (97.60)\n",
      "Generation # 440. Train Loss: 0.16. Train Acc (Test Acc): 95.00 (97.60)\n",
      "Generation # 445. Train Loss: 0.04. Train Acc (Test Acc): 98.00 (98.80)\n",
      "Generation # 450. Train Loss: 0.10. Train Acc (Test Acc): 96.00 (98.60)\n",
      "Generation # 455. Train Loss: 0.10. Train Acc (Test Acc): 95.00 (97.40)\n",
      "Generation # 460. Train Loss: 0.04. Train Acc (Test Acc): 99.00 (98.00)\n",
      "Generation # 465. Train Loss: 0.06. Train Acc (Test Acc): 97.00 (98.20)\n",
      "Generation # 470. Train Loss: 0.06. Train Acc (Test Acc): 98.00 (98.00)\n",
      "Generation # 475. Train Loss: 0.09. Train Acc (Test Acc): 96.00 (98.60)\n",
      "Generation # 480. Train Loss: 0.10. Train Acc (Test Acc): 97.00 (97.80)\n",
      "Generation # 485. Train Loss: 0.05. Train Acc (Test Acc): 99.00 (94.80)\n"
     ]
    }
   ],
   "source": [
    "# Start training loop\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "for i in range(generations):\n",
    "    rand_index = np.random.choice(len(train_xdata), size=batch_size)\n",
    "    rand_x = train_xdata[rand_index]\n",
    "    rand_x = np.expand_dims(rand_x, 3)\n",
    "    rand_y = train_labels[rand_index]\n",
    "    train_dict = {x_input: rand_x, y_target: rand_y}\n",
    "    \n",
    "    sess.run(train_step, feed_dict=train_dict)\n",
    "    temp_train_loss, temp_train_preds = sess.run([loss, prediction], feed_dict=train_dict)\n",
    "    temp_train_acc = get_accuracy(temp_train_preds, rand_y)\n",
    "    \n",
    "    if (i+1) % eval_every == 0:\n",
    "        eval_index = np.random.choice(len(test_xdata), size=evaluation_size)\n",
    "        eval_x = test_xdata[eval_index]\n",
    "        eval_x = np.expand_dims(eval_x, 3)\n",
    "        eval_y = test_labels[eval_index]\n",
    "        test_dict = {eval_input: eval_x, eval_target: eval_y}\n",
    "        test_preds = sess.run(test_prediction, feed_dict=test_dict)\n",
    "        temp_test_acc = get_accuracy(test_preds, eval_y)\n",
    "        \n",
    "        # Record and print results\n",
    "        train_loss.append(temp_train_loss)\n",
    "        train_acc.append(temp_train_acc)\n",
    "        test_acc.append(temp_test_acc)\n",
    "        acc_and_loss = [(i+1), temp_train_loss, temp_train_acc, temp_test_acc]\n",
    "        acc_and_loss = [np.round(x,2) for x in acc_and_loss]\n",
    "        print('Generation # {}. Train Loss: {:.2f}. Train Acc (Test Acc): {:.2f} ({:.2f})'.format(*acc_and_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matlotlib code to plot the loss and accuracies\n",
    "eval_indices = range(0, generations, eval_every)\n",
    "# Plot loss over time\n",
    "plt.plot(eval_indices, train_loss, 'k-')\n",
    "plt.title('Softmax Loss per Generation')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Softmax Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train and test accuracy\n",
    "plt.plot(eval_indices, train_acc, 'k-', label='Train Set Accuracy')\n",
    "plt.plot(eval_indices, test_acc, 'r--', label='Test Set Accuracy')\n",
    "plt.title('Train and Test Accuracy')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
